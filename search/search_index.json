{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This is meant to be a repository of common high-throughput screening (HTS) tasks using the python programming language. It's probably somewhere in-between an introductory tutorial and a cookbook.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>This is going to assume some basic familiarity with python, such as what a function is, the basic data types such as integers, lists and dictionaries. The aim isn't to teach you python from scratch, but to teach python beginners how they can apply their new python knowledge to help them analyse HTS data.</p> <p>It's also going to assume you're somewhat familiar with typical HTS workflows and terminology, such as multi-well plates, IC50 values, z-scores etc.</p>"},{"location":"#python","title":"Python","text":"<p>The code in these tutorials uses python 3.10 and python's type-hints. This is optional, and the code will run the same without it. I prefer it for clarity and better IDE experience.</p> <p>The code snippets are written with an aim of clarity rather than numerical stability or handling edge-cases. Dealing with missing data or dividing by zero issues is a separate topic.</p>"},{"location":"#libraries","title":"Libraries","text":"<p>The code snippets will use a few external libraries, mainly from the scipy ecosystem:</p> <ul> <li>pandas</li> <li>numpy</li> <li>matplotlib</li> <li>scipy</li> </ul>"},{"location":"plates/","title":"Plates","text":"<p>If we consider a single plate, we can represent it in 2 main ways:</p> <ul> <li>A mapping of well-labels to values</li> <li>A 2D array representing values in the dimensions of a plate.</li> </ul>"},{"location":"plates/#mapping-well-label-to-values","title":"Mapping well label to values","text":""},{"location":"plates/#dataframe","title":"DataFrame","text":"<pre><code>from string import ascii_uppercase\nimport pandas as pd\nimport numpy as np\n# make well labels\nrows = ascii_uppercase[:16]\ncols = range(1, 25)\nwells = [f\"{r}{c:02}\" for r in rows for c in cols]\nvalues = np.random.randn(384)  # just random data\nplate_df = pd.DataFrame({\"well\": wells, \"val\": values})\nplate_df.head()\n</code></pre> well val 0 A01 0.454 1 A02 0.195 2 A03 1.639 3 A04 0.831 4 A05 0.277 ... <p>Dataframes are probably the most useful format as pandas enables easy transformations such as merging, filtering, sorting etc.</p>"},{"location":"plates/#dictionary","title":"Dictionary","text":"<pre><code>plate_dict = {well: val for well, val in zip(wells, values)}\n# print out first couple of well:value pairs\nfor well, value in list(plate_dict.items())[:5]:\nprint(well, value, sep=\": \")\n</code></pre> <pre><code>&gt; A01: -0.5348518247234555\n&gt; A02: -1.0802744081971545\n&gt; A03: -0.23594497461538594\n&gt; A04: -0.4214463113386894\n&gt; A05: -0.37864853878757504\n</code></pre> <p>Or look up a value for a single well <pre><code>plate_dict[\"A01\"]\n&gt; -0.5348518247234555\n</code></pre></p>"},{"location":"plates/#plate-arrays","title":"Plate arrays","text":"<p>If we want to convert a Dataframe of a plate into a 2D array:</p> <pre><code>plate_arr = np.empty((16, 24), dtype=float)\nplate_arr[:] = np.nan  # set all values to missing\nfor _, well, value in plate_df.itertuples():\n# get row, column indices from well label (0 indexed)\nrow_pos = ord(well[0])-65\ncol_pos = int(well[1:]) - 1\n# assign value to this index in array\nplate_arr[row_pos, col_pos] = value\n</code></pre>"},{"location":"plates/#plotting-a-plate","title":"Plotting a plate","text":"<p>With plates in their 2D array format, it is now very easy to plot these as a platemap with matplotlib.</p> <p><pre><code>import matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=[8, 5])\nplt.imshow(plate_arr, origin=\"upper\", cmap=\"bwr\")\nplt.colorbar(shrink=0.6)\n# display row and column labels\nplt.yticks(range(16), ascii_uppercase[:16], fontfamily=\"monospace\")\nplt.xticks(range(24), [f\"{i:02}\" for i in range(1, 25)], fontfamily=\"monospace\")\n# move column labels to top\nax.xaxis.tick_top()\nplt.show()\n</code></pre> </p>"},{"location":"wells/","title":"Wells","text":""},{"location":"wells/#creating-well-labels-from-row-and-column-positions","title":"Creating well labels from row and column positions","text":"<p>If we have row and column positions, we can convert these into well-labels.</p> <pre><code>from string import ascii_uppercase\ndef row_col_to_well(row: int, col: int) -&gt; str:\nreturn f\"{ascii_uppercase[row-1]}{col:02}\" \n</code></pre> <pre><code>row_col_to_well(1, 1)\n&gt; \"A01\"\n</code></pre>"},{"location":"wells/#row-and-column-positions-from-well-labels","title":"Row and column positions from well labels","text":"<pre><code>def well_to_row_col(well: str) -&gt; tuple[int, int]:\nrow = ord(well[0].upper()) - 64\ncol = int(well[1:])\nreturn row, col\n</code></pre> <pre><code>well_to_row_col(\"A01\")\n&gt; (1, 1)\n</code></pre> <p>The strange thing here is <code>ord(..) - 64</code> , what is ord, and why subtract 64? <code>ord()</code> converts a character into it's unicode integer representation, and for unimportant reasons \"A\" is 65. So if you want A = 1, B = 2, etc, subtract 64.</p> <p>Instead of just returning a tuple, we can use a <code>NamedTuple</code>, so we can access the row and column values by name, rather than position.</p> <pre><code>from typing import NamedTuple\nclass Well(NamedTuple):\nlabel: str\nrow: int\ncolumn: int\ndef make_well(well: str) -&gt; Well:\nrow, column = well_to_row_col(well)\nreturn Well(well, row, column)\n</code></pre> <pre><code>make_well(\"A01\")\n&gt;  Well(label='A01', row=1, column=1)\n</code></pre>"},{"location":"wells/#zero-padding-well-labels","title":"Zero padding well labels","text":"<p>Wells are typically zero-padded, such as \"A01\" rather than \"A1\", but we sometimes have to convert well labels between the two.</p> <p>This uses python string formatting, which can handily format integers as strings with zero padding for us.</p> <pre><code>def pad_well(well: str) -&gt; str:\nrow = well[0]\ncol = int(well[1:])\nreturn f\"{row}{col:02}\"\n</code></pre> <pre><code>pad_well(\"A1\")\n&gt; \"A01\"\n</code></pre> <p>So the magic is done in <code>{col:02}</code>. This means the <code>col</code> number, and everything after the colon is the fomatting specifications, in this case we want 2 digits, zero-padded if necessary <code>:02</code>.</p> <p>Unpadding wells is almost identical, except we don't specify any special formatting in <code>{col}</code>.</p> <pre><code>def unpad_well(well: str) -&gt; str:\nrow = well[0]\ncol = int(well[1:])\nreturn f\"{row}{col}\"\n</code></pre> <pre><code>unpad_well(\"A01\")\n&gt; \"A1\"\n</code></pre>"},{"location":"wells/#generating-a-well-labels","title":"Generating a well labels","text":"<p>If we wanted to generate all the well labels in a 384-well plate: <pre><code>from string import ascii_uppercase\nwells = [f\"{r}{c:02}\" for r in ascii_uppercase[:16] for c in range(1, 25)]\n</code></pre></p> <p>It's worth noting that python's <code>range(1, 25)</code> function is not inclusive of the final number in the sequence, that's why we went to 25 and not 24.</p>"},{"location":"wells/#wells-from-1536-well-plates","title":"Wells from 1536 well plates","text":"<p>A 1536 well plate has 48 columns and 32 rows. The key difference is the 32 rows which cannot be represented as a single alphabetical character. The most common 1536 well plate formats have rows which range from A-Z, and then continue from AA until AF. So our previous python functions will break with wells from a 1536-well plate.</p> <p></p> <pre><code>from string import ascii_uppercase\ndef row_col_to_well(row: int, col: int) -&gt; str:\nif row &lt;= 26:\nrow_letter = ascii_uppercase[row - 1]\nelse:\nrow_letter = \"A\" + ascii_uppercase[row % (26 + 1)]\nreturn f\"{row_letter}{col:02}\"\n</code></pre> <p>We now can't assume the first character of the well label corresponds to the row, it might be one character (A-Z), or 2 character (AA-AF). But we can index  the well label starting from the end, so we never have to worry about that. So the last 2 characters (<code>well[-2:]</code>) are always the column (if zero-padded), and from there until the start of the well-label (<code>well[:-2]</code>) is the row, whether it's 1 character or 2.</p> <p>We then need to convert the row character(s) into an integer. Before we used the <code>ord()</code> function to convert the character into a numerical representation. Now we have to handle either \"A\" or \"AA\", which means either row 1 or row 27. The simplest way is to check the length, if it's only 1 character, treat it the usual way, if it's 2 characters, get the number of the last character and add 26 (or to save an operation, subtract 38 instead of the usual 64 from the <code>ord()</code> function).</p> <pre><code>def well_to_row_col(well: str) -&gt; tuple[int, int]:\ncol = int(well[-2:])  # last 2 characters as an integer\nrow_char = well[:-2]  # up until the last 2 characters\nif len(row_char) == 1:\nrow = ord(row_char.upper()) - 64\nelse:\nrow = ord(row_char[-1].upper()) - 38  # (65 - 26 = 38)\nreturn row, col\n</code></pre> <p>We could also solve this issue using regular expressions but they're usually more trouble than they're worth.</p>"},{"location":"wells/#sorting-1536-well-labels","title":"Sorting 1536 well labels","text":"<p>With 96 and 384 well plates we can sort well-labels alphabetically, however with 1536 well plates this does not work as expected. One way around this is to extract the row and column indices from the well label first, then sort by row and column.</p> <pre><code># construct example data\nwells = [\"A01\", \"AA01\", \"AF01\", \"Z01\"]\nvals = [1, 2, 3, 4]\ndf = pd.DataFrame({\"well\": wells, \"data\": vals})\n# generate row and column labels\nrows, cols = zip(*[well_to_row_col(i) for i in df[\"well\"]])\ndf[\"row\"] = rows\ndf[\"col\"] = cols\n# sort data by row and column\ndf_sorted = df.sort_values([\"row\", \"col\"])\n# maybe drop \"row\" and \"col\" columns\n</code></pre>"},{"location":"conc_response_curves/fitting/","title":"Fitting concentration response curves","text":"<p>Fitting dose or concentration response curves is a common but nuanced task. To do this in python we can define the curve fitting function in numpy, and use <code>scipy.optimize.curve_fit()</code> to fit a curve to our data.</p>"},{"location":"conc_response_curves/fitting/#3-parameter-curve","title":"3 parameter curve","text":""},{"location":"conc_response_curves/fitting/#concentration","title":"Concentration","text":"<pre><code>import numpy as np\ndef hill_3_param(\nx: np.ndarray, top: float, botton: float, ec50: float\n) -&gt; np.ndarray:\nreturn bottom + x * (top - bottom) / (ec50 + x)\n</code></pre>"},{"location":"conc_response_curves/fitting/#log-concentrations","title":"log concentrations","text":"<pre><code>def hill_3_param(\nx: np.ndarray, top: float, botton: float, log_ec50: float\n) -&gt; np.ndarray:\nreturn bottom + (top - bottom)  / (1 + 10**(log_ec50 - x))\n</code></pre>"},{"location":"conc_response_curves/fitting/#4-parameter-curve","title":"4 parameter curve","text":""},{"location":"conc_response_curves/fitting/#concentration_1","title":"Concentration","text":"<pre><code>import numpy as np\ndef hill_4_param(\nx: np.ndarray, top: float, bottom: float, ec50: float, hillslope: float\n) -&gt; np.ndarray:\nnumerator = bottom + (x**hillslope) * (top - bottom)\ndenominator = ((x**hillslope) + (ec50**hillslope))\nreturn numerator / denominator\n</code></pre>"},{"location":"conc_response_curves/fitting/#log-concentrations_1","title":"log concentrations","text":"<pre><code>def hill_4_param(\nx: np.ndarray, top: float, bottom: float, log_ec50: float, hillslope: float\n) -&gt; np.ndarray:\nreturn bottom + (top - bottom) / (1 + 10**((log_ec50 - x) * hillslope))\n</code></pre>"},{"location":"conc_response_curves/fitting/#fitting-the-model","title":"Fitting the model","text":"<p>At it's simplest, assuming <code>x</code> is an array of concentrations and <code>y</code> is an array of corresponding responses, we can run: <pre><code>popt, pcov = scipy.optimize.curve_fit(hill_3_param, x, y)\n</code></pre></p> <p>This returns <code>popt</code>, which are the fitted [<code>top</code>, <code>bottom</code>, <code>ec50</code>, <code>hillslope</code> (if 4 parameter)] values for your model. These can be used directly such as the \\(EC_{50}\\), or used with interpolated data to plot a curve.</p> <p><code>pcov</code> is the covariance of your parameter estimates which can be used to calculate the standard deviation of your \\(EC_{50}\\) for instance.</p>"},{"location":"conc_response_curves/fitting/#adjusting-the-curve-fitting","title":"Adjusting the curve fitting","text":"<p>You might find the curve fitting hasn't worked particularly well with the default settings, in this case it often helps to adjust some parameters in <code>scipy.optimize.curve_fit()</code>.</p>"},{"location":"conc_response_curves/fitting/#p0","title":"<code>p0</code>","text":"<p><code>p0</code> serves as the initial starting point for your model parameters. The closer these are expected values, the easier the job the curve fitting algorithm will have. If you don't specify starting points for <code>p0</code>, they will start as 1 for all values.</p> <pre><code>p0 = [100, 0, 1e-3]  # starting point for [top, bottom, ec50] params\npopt, pcov = scipy.optimize.curve_fit(hill_3_param, x, y, p0=p0)\n</code></pre>"},{"location":"conc_response_curves/fitting/#bounds","title":"<code>bounds</code>","text":"<p>You often have some idea if the parameters the curve fitting produces are at all possible given your assay. For example a negative hill-slope, or a <code>bottom</code> values less than 0. We can specify upper and lower bounds in the curve fitting with the <code>bounds</code> argument. These should be fairly relaxed, but within the realms of possibility.</p> <pre><code>p0 = [100, 0, 1e-3]\nbounds = (\n(0, 0, 0),     # lower bounds for [top, bottom, ec50]\n(300, 300, 1)  # upper bounds for [top, bottom, ec50]\n)\npopt, pcov = scipy.optimize.curve_fit(hill_3_param, x, y, p0=p0, bounds=bounds)\n</code></pre> <p>If you want to specify bounds for some parameters but leave others boundless, use <code>-np.inf</code> and <code>np.inf</code>.</p> <p>E.g here we set upper and lower bounds for the <code>top</code> and <code>bottom</code> parameters, but allow <code>ec50</code> to be fit to any value. <pre><code>bounds = (\n(0, 0, -np.inf),     \n(300, 300, np.inf) \n)\npopt, pcov = scipy.optimize.curve_fit(hill_3_param, x, y, bounds=bounds)\n</code></pre></p> <p>Note</p> <p>If you set <code>bounds</code>, you should be aware of the default values of <code>p0</code> being initialised as 1. If your bounds do not cover 1 then your model will fail to fit. So it's probably wise to set values for <code>p0</code> if you're also setting <code>bounds</code>.</p>"},{"location":"conc_response_curves/fitting/#pcov-and-parameter-uncertainty","title":"<code>pcov</code> and parameter uncertainty","text":"<p>The <code>pcov</code> object returned on <code>scipy.optimize.curve_fit()</code> is an n by n matrix of estimated parameter covariances, where n is the number of parameters in your model. We can use the diagonal of this matrix to get the estimated standard deviation of our parameters.</p> <pre><code>popt, pcov = scipy.optimize.curve_fit(hill_3_param, x, y)\np_err = np.sqrt(np.diag(pcov))\n</code></pre> <p>Since our <code>hill_3_param</code> model has 3 parameters, <code>p_err</code> will now the variance of those 3 parameters in the same order as their arguments in the original function (top, bottom, ec50).</p> <pre><code>print(f\"EC50 = {popt[2]} ({p_err[2]} std. dev.)\")\n</code></pre>"},{"location":"conc_response_curves/plotting/","title":"Plotting concentration response curves","text":"<p>Using example data from here, we'll fit and plot a 3 parameter concentration response curve.</p>"},{"location":"conc_response_curves/plotting/#generating-interpolated-points","title":"Generating interpolated points","text":"<p>Given our parameters from our fitted model, we can now generate an estimated response value for any given concentration.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom scipy.optimize import curve_fit\ndef hill_3_param(\nx: np.ndarray, top: float, botton: float, ec50: float\n) -&gt; np.ndarray:\nreturn bottom + x * (top - bottom) / (ec50 + x)\n# read in example concentration response data\ndf = pd.read_csv(\"conc_response.csv\").dropna()\ndf_a = df[df[\"drug\"] == \"A\"]\nconc = df_a[\"conc\"].to_numpy()\nresponse = df_a[\"response\"].to_numpy()\npopt, pcov = curve_fit(hill_3_param, x=con, y=response)\ntop, bottom, ec50 = popt\n</code></pre> <p>We now have our parameters of our 3 parameter model (top, bottom, ec50): <pre><code>[4.05044597e+01 3.22471271e-01 1.10047765e-07]\n</code></pre></p> <p>Now we generate 100 interpolated values between our minimum and maximum concentration values to plot a smoother curve.</p> <pre><code>x_interpolated = np.logspace(min(np.log10(conc)), max(np.log(conc)), 100)\ny_interpolated = hill_3_param(x_interpolated, *popt)\n</code></pre>"},{"location":"conc_response_curves/plotting/#plotting-with-matplotlib","title":"Plotting with matplotlib","text":"<p>Now we can plot both the raw data points and the fitted curve in matplotlib.</p> <pre><code>plt.plot(x_interpolated, y_interpolated, label=\"fitted curve\", color=\"black\")\nplt.scatter(\nconc, response, label=\"raw data\", color=\"black\", facecolor=\"white\", zorder=99\n)\nplt.vlines(x=ec50, ymin=0, ymax=top/2, linestyle=\"dotted\", color=\"gray\")\nplt.hlines(y=top/2, xmin=min(conc), xmax=ec50, linestyle=\"dotted\", color=\"gray\")\nplt.text(x=1e-9, y=23, s=f\"$EC_{{50}} = {ec50:.2E}$\")\nplt.xlabel(\"Concentration\")\nplt.ylabel(\"Response\")\nplt.xscale(\"log\")\nplt.legend()\n</code></pre> <p></p>"},{"location":"conc_response_curves/plotting/#plotting-standard-deviations-with-pcov","title":"Plotting standard deviations with <code>pcov</code>","text":"<p>As mentioned in the curve fitting section we can get the standard deviations of our curve parameters from the <code>pcov</code> matrix.</p> <pre><code>p_err = np.sqrt(np.diag(pcov))\n</code></pre> <p>We can then plot this uncertainty on the curve by calculating 2 additional curves, one with our parameters + standard deviation, and another - standard deviation.</p> <pre><code>y_hat_upper = hill_3_param(\nx_interpolated, top+p_err[0], bottom+p_err[1], ec50+p_err[2]\n)\ny_hat_lower = hill_3_param(\nx_interpolated, top-p_err[0], bottom-p_err[1], ec50-p_err[2]\n)\nplt.figure(figsize=[10, 6])\nplt.plot(x_interpolated, y_hat, label=\"fitted curve\", color=\"black\")\nplt.scatter(\nx, y, s=50,label=\"raw data\", color=\"black\", facecolor=\"white\", zorder=999\n)\nplt.fill_between(\nx_interpolated,\ny1=y_hat_upper,\ny2=y_hat_lower,\ncolor=\"gray\",\nalpha=0.3,\nlabel=\"$\\pm$ 1SD\"\n)\nplt.vlines(x=ec50, ymin=0, ymax=top/2, linestyle=\"dotted\", color=\"gray\")\nplt.hlines(y=top/2, xmin=min(x), xmax=ec50, linestyle=\"dotted\", color=\"gray\")\nplt.text(x=1e-9, y=21, s=f\"$EC_{{50}}$ = {ec50:.2E}\")\nplt.xlabel(\"Concentration\")\nplt.ylabel(\"Response\")\nplt.xscale(\"log\")\nplt.legend()\n</code></pre> <p></p>"},{"location":"hit_selection/ssmd/","title":"Strictly Standardised Mean Difference (SSMD)","text":"<p>SSMD is a measure of effects size and can be used for both quality control like Z-prime, as well as hit-selection. It was initially prosed by Zhang.</p> \\[ \\beta = \\frac{\\mu_1 - \\mu_2}{\\sqrt{\\sigma_1^2 + \\sigma_2^2 - 2\\sigma_{12}}} \\]"},{"location":"hit_selection/z_score/","title":"Z-score","text":"<p>The Z-score can be used for both standardisation of values as well as hit-selection.</p> <p>At it's most basic, it's subtracting the mean of a population (\\(\\mu\\)) and dividing by the standard deviation of the same population (\\(\\sigma\\)), resulting in values centered at 0, and Z being the number of standard deviations away from the population centre.</p> \\[ Z = \\frac{x - \\mu_x}{\\sigma_x} \\] <pre><code>def z_score(x: np.ndarray) -&gt; np.ndarray:\nreturn (x - np.mean(x)) / np.std(x)\n</code></pre>"},{"location":"hit_selection/z_score/#robust-z-score","title":"robust Z-score","text":"<p>The robust z-score is similar to the typical z-score, but is less sensitive to outliers. It uses the median and median absolute deviation instead of mean and standard deviation.</p> <pre><code>import numpy as np\nfrom scipy.stats import median_abs_deviation as mad\ndef robust_z_score(x: np.ndarray) -&gt; np.ndarray:\nreturn (x - np.median(x)) / mad(x)\n</code></pre>"},{"location":"hit_selection/z_score/#z-score-using-control-values","title":"Z-score using control values","text":"<p>A common metric in HTS is to use the z-score, but with the mean and standard deviation of the neutral control wells instead of the population mean and standard deviation.</p> <pre><code>def z_score_to_controls(x: np.ndarray, controls: np.ndarray) -&gt; np.ndarray:\nreturn (x - np.mean(controls)) / np.std(controls)\n</code></pre>"},{"location":"hit_selection/z_score/#from-a-dataframe","title":"From a dataframe","text":"<p>If we have a dataframe with columns of readout and annotations we can write a function that will subset the data for us to calculate the Z-score against the control values.</p> Cell_Count Metadata_Compound_Name 3987 ABC00001 4985 ABC00002 4590 ABC00003 3098 ABC00004 4909 DMSO <pre><code>import pandas as pd\ndef z_score_to_controls_df(\ndf: pd.DataFrame,\nfeature_cols: list[str],\ncontrol_column_name: string,\ncontrol_name: string,\n) -&gt; pd.DataFrame:\ndf_copy = df.copy()\ncontrol_df = df[df[control_column] == control_name]\nmu = control_df[feature_cols].mean()\nsigma = control_df[feature_cols].std()\ndf_copy[feture_cols] = (df[feature_cols] - mu) / sigma\nreturn df_copy\ndf_zscored = z_score_to_controls_df(\ndf = df,\nfeature_cols = [\"Cell_Count\"],\ncontrol_column_name = \"Metadata_Compound_Name\",\ncontrol_name = \"DMSO\"\n)\n</code></pre>"},{"location":"quality_control/z_prime/","title":"Z prime","text":""},{"location":"quality_control/z_prime/#standard-z","title":"Standard Z`","text":"<p>Z prime is used as a quality control metric to measure the separation between positive and negative controls.</p> \\[ {\\displaystyle {\\text{Z`}}=1-{3(\\sigma _{p}+\\sigma _{n}) \\over |\\mu _{p}-\\mu _{n}|}} \\] <p>Where \\(\\mu_p\\) and \\(\\mu_n\\) are the mean of the positive and negative controls, and \\(\\sigma_p\\) and \\(\\sigma_n\\) are the standard deviations of the positive and negative controls. A value greater than 0.5 is typically considered good.</p> <p>To calculate the Z` between a list of positive control values and negative control values: <pre><code>import numpy as np\ndef z_prime(pos: list, neg: list) -&gt; float:\nsigma_p, sigma_n = np.std(pos), np.std(neg)\nmu_p, mu_n = np.mean(pos), np.mean(neg)\nreturn 1 - (3 * (sigma_p + sigma_n) / np.abs(mu_p - mu_n))\n</code></pre></p>"},{"location":"quality_control/z_prime/#robust-z","title":"Robust Z`","text":"<p>The robust Z prime is the same calculation as before, but using the median instead of mean, and median absolute deviation instead of standard deviation.</p> <pre><code>import numpy as np\nfrom scipy.stats import median_abs_deviation as mad\ndef robust_z_prime(pos: list, neg: list) -&gt; float:\nsigma_p, sigma_n = mad(pos), mad(neg)\nmu_p, mu_n = np.median(pos), np.median(neg)\nreturn 1 - (3 * (sigma_p + sigma_n) / np.abs(mu_p - mu_n))\n</code></pre>"}]}